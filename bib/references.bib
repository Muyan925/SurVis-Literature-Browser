@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020},
  publisher={arXiv},
    keywords={
    type: Foundational Models & Adversarial Attacks,
    bidirectional transformer,
    masked language modeling,
    next sentence prediction,
    pre-training,
    fine-tuning,
    GLUE benchmark,
    SQuAD,
    contextual embeddings
  },

  abstract={Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training
on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic
in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language task from only
a few examples or from simple instructions - something which current NLP systems still largely
struggle to do. Here we show that scaling up language models greatly improves task-agnostic,
few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion
parameters, 10x more than any previous non-sparse language model, and test its performance in
the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,
with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation, question-answering, and
cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as
unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same
time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some
datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,
we find that GPT-3 can generate samples of news articles which human evaluators have difficulty
distinguishing from articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general.},
  series={None},
  doi={10.48550/arXiv.2005.14165}
}

@article{lee2018pre,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Lee, JDMCK and Toutanova, K},
  journal={arXiv preprint arXiv:2005.14165},
  year={2018},
  publisher={arXiv},
  keywords={
    type: Foundational Models & Adversarial Attacks,
    bidirectional transformer,
    masked language modeling,
    next sentence prediction,
    pre-training,
    fine-tuning,
    GLUE benchmark,
    SQuAD,
    contextual embeddings
  },
  abstract={We introduce a new language representation model called BERT, which stands for
Bidirectional Encoder Representations from
Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from
unlabeled text by jointly conditioning on both
left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer
to create state-of-the-art models for a wide
range of tasks, such as question answering and
language inference, without substantial task specific architecture modifications.
BERT is conceptually simple and empirically
powerful. It obtains new state-of-the-art results on eleven natural language processing
tasks, including pushing the GLUE score to
80.5% (7.7% point absolute improvement),
MultiNLI accuracy to 86.7% (4.6% absolute
improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).},
  series={None},
  doi={10.48550/arXiv.1810.04805}
}

@article{xu2024large,
  title={Large language models for cyber security: A systematic literature review},
  author={Xu, HanXiang and Wang, ShenAo and Li, Ningke and Wang, Kailong and Zhao, Yanjie and Chen, Kai and Yu, Ting and Liu, Yang and Wang, HaoYu},
  journal={arXiv preprint arXiv:2405.04760},
  year={2024},
  publisher={arXiv},
  keywords={
    type: Software and System Security,
    systematic literature review,
    cybersecurity tasks,
    vulnerability detection,
    malware analysis,
    domain adaptation,
    LLM evaluation metrics,
    dataset challenges,
    fine-tuning,
    prompt engineering
  },
  abstract={The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in a
variety of application domains, including cybersecurity. As the volume and sophistication of cyber threats continue to grow, there
is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks.
In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity (LLM4Security).
By comprehensively collecting over 30K relevant papers and systematically analyzing 127 papers from top security and software
engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity
domain.
Through our analysis, we identify several key findings. First, we observe that LLMs are being applied to a wide range of cybersecurity
tasks, including vulnerability detection, malware analysis, network intrusion detection, and phishing detection. Second, we find
that the datasets used for training and evaluating LLMs in these tasks are often limited in size and diversity, highlighting the
need for more comprehensive and representative datasets. Third, we identify several promising techniques for adapting LLMs to
specific cybersecurity domains, such as fine-tuning, transfer learning, and domain-specific pre-training. Finally, we discuss the main
challenges and opportunities for future research in LLM4Security, including the need for more interpretable and explainable models,
the importance of addressing data privacy and security concerns, and the potential for leveraging LLMs for proactive defense and
threat hunting.
Overall, our survey provides a comprehensive overview of the current state-of-the-art in LLM4Security and identifies several
promising directions for future research. We believe that the insights and findings presented in this survey will contribute to the
growing body of knowledge on the application of LLMs in cybersecurity and provide valuable guidance for researchers and practitioners
working in this field.},
  series={None},
  doi={10.48550/arXiv.2405.04760}
}

@article{nguyen2025towards,
  title={Towards effective identification of attack techniques in cyber threat intelligence reports using large language models},
  author={Nguyen, Hoang Cuong and Tariq, Shahroz and Chhetri, Mohan Baruwal and Vo, Bao Quo},
  journal={arXiv preprint arXiv:2505.03147},
  year={2025},
  publisher={arXiv},
  keywords={type: Network Security,Cyber Threat Intelligence, LLMs, Text Summarisation, Cybersecurity, MITRE ATT&CK Techniques, Cyber Reports},
  abstract={This work evaluates the performance of Cyber Threat Intelligence
(CTI) extraction methods in identifying attack techniques from
threat reports available on the web using the MITRE ATT&CK
framework. We analyse four configurations utilising state-of-theart tools, including the Threat Report ATT&CK Mapper (TRAM)
and open-source Large Language Models (LLMs) such as Llama2.
Our findings reveal significant challenges, including class imbalance, overfitting, and domain-specific complexity, which impede
accurate technique extraction. To mitigate these issues, we propose
a novel two-step pipeline: first, an LLM summarises the reports, and
second, a retrained SciBERT model processes a rebalanced dataset
augmented with LLM-generated data. This approach achieves an improvement in F1-scores compared to baseline models, with several
attack techniques surpassing an F1-score of 0.90. Our contributions enhance the efficiency of web-based CTI systems and support
collaborative cybersecurity operations in an interconnected digital landscape, paving the way for future research on integrating
human-AI collaboration platforms.},
   series={None},
  doi={10.48550/arXiv.2505.03147}
}

@article{karbab2019maldy,
  title={Maldy: Portable, data-driven malware detection using natural language processing and machine learning techniques on behavioral analysis reports},
  author={Karbab, ElMouatez Billah and Debbabi, Mourad},
  journal={Digital Investigation},
  volume={28},
  pages={S77--S87},
  year={2019},
  publisher={Elsevier},
  keywords={type: Software & System Security, Malware,Android, Win32,Behavioral analysis,Machine learning, NLP},
  abstract={n response to the volume and sophistication of malicious software or malware, security investigators
rely on dynamic analysis for malware detection to thwart obfuscation and packing issues. Dynamic
analysis is the process of executing binary samples to produce reports that summarise their runtime
behaviors. The investigator uses these reports to detect malware and attribute threat types leveraging
manually chosen features. However, the diversity of malware and the execution environments make
manual approaches not scalable because the investigator needs to manually engineer fingerprinting
features for new environments. In this paper, we propose, MalDy (mal die), a portable (plug and play)
malware detection and family threat attribution framework using supervised machine learning techniques. The key idea of MalDy portability is the modeling of the behavioral reports into a sequence of
words, along with advanced natural language processing (NLP) and machine learning (ML) techniques
for automatic engineering of relevant security features to detect and attribute malware without the
investigator intervention. More precisely, we propose to use bag-of-words (BoW) NLP model to formulate
the behavioral reports. Afterward, we build ML ensembles on top of BoW features. We extensively
evaluate MalDy on various datasets from different platforms (Android and Win32) and execution environments. The evaluation shows the effectiveness and the portability of MalDy across the spectrum of
the analyses and settings.},
  series={None},
  doi={10.1016/j.diin.2019.01.017}
}

@article{ucci2019survey,
  title={Survey of machine learning techniques for malware analysis},
  author={Ucci, Daniele and Aniello, Leonardo and Baldoni, Roberto},
  journal={Computers \& Security},
  volume={81},
  pages={123--147},
  year={2019},
  publisher={Elsevier},
  keywords={type: Software & System Security, Portable executable, Malware analysis, Machine learning, Benchmark,Malware analysis economics},
  abstract={Coping with malware is getting more and more challenging, given their relentless growth in
complexity and volume. One of the most common approaches in literature is using machine
learning techniques, to automatically learn models and patterns behind such complexity,
and to develop technologies to keep pace with malware evolution. This survey aims at providing an overview on the way machine learning has been used so far in the context of
malware analysis in Windows environments, i.e. for the analysis of Portable Executables.
We systematize surveyed papers according to their objectives (i.e., the expected output), what
information about malware they specifically use (i.e., the features), and what machine learning techniques they employ (i.e., what algorithm is used to process the input and produce
the output). We also outline a number of issues and challenges, including those concerning
the used datasets, and identify the main current topical trends and how to possibly advance
them. In particular, we introduce the novel concept of malware analysis economics, regarding
the study of existing trade-offs among key metrics, such as analysis accuracy and economical costs.},
  series={None},
  doi={10.1016/j.cose.2018.11.001}
}

@article{ferrag2024revolutionizing,
  title={Revolutionizing cyber threat detection with large language models: A privacy-preserving bert-based lightweight model for iot/iiot devices},
  author={Ferrag, Mohamed Amine and Ndhlovu, Mthandazo and Tihanyi, Norbert and Cordeiro, Lucas C and Debbah, Merouane and Lestable, Thierry and Thandi, Narinderjit Singh},
  journal={IEEE Access},
  volume={12},
  pages={23733--23750},
  year={2024},
  publisher={IEEE},
  keywords={type:  Network Security, Cyber threat detection, IoT networks, generative AI, BERT, large language models},
  abstract={The field of Natural Language Processing (NLP) is currently undergoing a revolutionary
transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking
Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the
importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting
in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks
with both high precision and minimal computational requirements. This paper presents SecurityBERT,
a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT)
model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a
novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE).
We effectively represented network traffic data in a structured format by combining PPFLE with the Bytelevel Byte-Pair Encoder (BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms
traditional Machine Learning (ML) and Deep Learning (DL) methods, such as Convolutional Neural
Networks (CNNs) or Recurrent Neural Networks (RNNs), in cyber threat detection. Employing the
Edge-IIoTset cybersecurity dataset, our experimental analysis shows that SecurityBERT achieved an
impressive 98.2% overall accuracy in identifying fourteen distinct attack types, surpassing previous records
set by hybrid solutions such as GAN-Transformer-based architectures and CNN-LSTM models. With an
inference time of less than 0.15 seconds on an average CPU and a compact model size of just 16.7MB,
SecurityBERT is ideally suited for real-life traffic analysis and a suitable choice for deployment on
resource-constrained IoT devices.},
  series={None},
  doi={10.1109/ACCESS.2024.3363469}
}


@article{kheddar2024transformers,
  title={Transformers and large language models for efficient intrusion detection systems: A comprehensive survey},
  author={Kheddar, Hamza},
  journal={arXiv preprint arXiv:2408.07583},
  year={2024},
  publisher={arXiv},
  keywords={type: Network Security, Anomalies detection, Cyber-security, Intrusion detection, Large language model, Natural language processing, Transformers},
  abstract={With significant advancements in Transformers and large language models (LLMs), natural language
processing (NLP) has extended its reach into many research fields due to its enhanced capabilities in text
generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In
cybersecurity, many parameters that need to be protected and exchanged between senders and receivers
are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures
of communication protocols. This survey paper provides a comprehensive analysis of the utilization
of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and
bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The
fundamentals of Transformers are discussed, including background information on various cyber-attacks
and datasets commonly used in this field. The survey explores the application of Transformers in intrusion
detection systems (IDSs), focusing on different architectures such as Attention-based models, LLMs like
BERT and GPT, CNN/LSTM-Transformer hybrids, and emerging approaches like Vision Transformers
(ViTs), and more. Furthermore, it explores the diverse environments and applications where Transformers
and LLMs-based IDS have been implemented, including computer networks, internet-of-things (IoT)
devices, critical infrastructure protection, cloud computing, software-defined networking (SDN), as well as
in autonomous vehicles (AVs). The paper also addresses research challenges and future directions in this
area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and
more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and
LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further
research and development.},
  series={None},
   doi={10.48550/arXiv.2408.07583}
}

@article{shayegani2023survey,
  title={Survey of vulnerabilities in large language models revealed by adversarial attacks},
  author={Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2310.10844},
  year={2023},
  publisher={arXiv},
  keywords={type: Foundational Models & Adversarial Attacks, adversarial attacks,jailbreak attacks,prompt injection,context contamination,black-box access,white-box access,LLM security},
  abstract={Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate
more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper
surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of
trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has
shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human
feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as
evidenced by the prevalence of ‘jailbreak’ attacks on models like ChatGPT and Bard. In this survey, we
first provide an overview of large language models, describe their safety alignment, and categorize existing
research based on various learning structures: textual-only attacks, multi-modal attacks, and additional
attack methods specifically targeting complex systems, such as federated learning or multi-agent systems.
We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities
and potential defenses. To make this field more accessible to newcomers, we present a systematic review of
existing works, a structured typology of adversarial attack concepts, and additional resources, including
slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational
Linguistics (ACL’24)},
  series={None},
  doi={10.48550/arXiv.2310.10844}
}

@article{motlagh2024large,
  title={Large language models in cybersecurity: State-of-the-art},
  author={Motlagh, Farzad Nourmohammadzadeh and Hajizadeh, Mehrdad and Majd, Mehryar and Najafi, Pejman and Cheng, Feng and Meinel, Christoph},
  author={Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2402.00891},
  year={2024},
  publisher={arXiv},
  keywords={type:  Foundational Models & Adversarial Attacks, LLM, Large Language Model, AI, cybersecurity, advanced threats, cyberattacks, cyberdefense, privacy and security},
  abstract={The rise of Large Language Models (LLMs) has revolutionized our
comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored
the applications of LLMs across diverse fields, significantly elevating
capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain.
This study examines the existing literature, providing a thorough
characterization of both defensive and adversarial applications of
LLMs within the realm of cybersecurity. Our review not only surveys
and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications,
we aim to provide a holistic understanding of the potential risks and
opportunities associated with LLM-driven cybersecurity},
  series={None},
  doi={10.48550/arXiv.2402.00891}
}

